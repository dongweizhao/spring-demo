<?xml version="1.0" encoding="UTF-8"?>
<configuration scan="true">
    <property name="log_dir" value="/data/web_log/java"/>
    <property name="moduleName" value="spring-user-jar-demo"/>
    <property name="default_log" value="${log_dir}/${moduleName}/${moduleName}-default"/>
    <property name="request_trace_log" value="${log_dir}/${moduleName}/${moduleName}-request-trace"/>
    <property name="error_log" value="${log_dir}/${moduleName}/common-error"/>
    <springProperty scope="context" name="log.level" source="logging.level.root" defaultValue="INFO"/>
    <!-- 日志格式 -->
    <property name="LOG_PATTERN"
              value="%date{HH:mm:ss.SSS}, [%X{X-B3-TraceId:-} %X{X-B3-SpanId:-}] [%thread] %-5level [%marker] %logger{36} %method:%L - %msg %n"/>

    <appender name="console" class="ch.qos.logback.core.ConsoleAppender">
        <encoder charset="UTF-8">
            <pattern>
                ${LOG_PATTERN}
            </pattern>
        </encoder>
    </appender>

    <!-- 默认日志 按小时切分 -->
    <!--&lt;!&ndash;参考https://logback.qos.ch/manual/appenders.html，如果fileNamePattern是以gz为结尾，那么会自动对历史日志进行gzip压缩，-->
    <!--TimeBasedRollingPolicy supports automatic file compression. This feature is enabled if the value of the fileNamePattern option ends with .gz or .zip.-->
    <!--错误日志时间保留长一点，保留30天，理论这个数据会比较小-->
    <appender name="common-error" class="ch.qos.logback.core.rolling.RollingFileAppender">
        <file>${error_log}.log</file>
        <rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">
            <fileNamePattern>${error_log}.%d{yyyy-MM-dd}.log.gz</fileNamePattern>
            <maxHistory>30</maxHistory>
            <totalSizeCap>20GB</totalSizeCap>
            <cleanHistoryOnStart>true</cleanHistoryOnStart>
        </rollingPolicy>
        <encoder>
            <pattern>
                ${LOG_PATTERN}
            </pattern>
        </encoder>
        <filter class="ch.qos.logback.classic.filter.LevelFilter">
            <level>ERROR</level>
            <onMatch>ACCEPT</onMatch>
            <onMismatch>DENY</onMismatch>
        </filter>
    </appender>

    <!--https://www.jianshu.com/p/09f55766088d-->
    <!--日志按照小时压缩，时间保留长一点，保留15天，理论这个数据会比较小-->
    <appender name="default" class="ch.qos.logback.core.rolling.RollingFileAppender">
        <file>${default_log}.log</file>
        <rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">
            <fileNamePattern>${default_log}.%d{yyyy-MM-dd}.log.gz</fileNamePattern>
            <maxHistory>360</maxHistory>
            <totalSizeCap>80GB</totalSizeCap>
            <cleanHistoryOnStart>true</cleanHistoryOnStart>
        </rollingPolicy>
        <encoder>
            <pattern>
                ${LOG_PATTERN}
            </pattern>
        </encoder>
    </appender>

    <!-- 异步输出 -->
    <appender name="async-default" class="ch.qos.logback.classic.AsyncAppender">
        <!-- 不丢失日志.默认的,如果队列的80%已满,则会丢弃TRACT、DEBUG、INFO级别的日志 -->
        <discardingThreshold>0</discardingThreshold>
        <!-- 更改默认的队列的深度,该值会影响性能.默认值为256 -->
        <!--https://logback.qos.ch/manual/appenders.html#AsyncAppender-->
        <queueSize>512</queueSize>
        <!-- 添加附加的appender,最多只能添加一个 -->
        <appender-ref ref="default"/>
        <includeCallerData>true</includeCallerData>
    </appender>

    <logger name="jdbc.connection" level="DEBUG" additivity="false">
        <appender-ref ref="common-error"/>
    </logger>
    <logger name="jdbc.sqlonly" level="OFF"/>
    <logger name="jdbc.resultset" level="OFF"/>
    <logger name="jdbc.audit" level="OFF"/>
    <logger name="org.springframework.kafka" level="INFO"/>
    <logger name="org.apache.kafka" level="INFO"/>
    <logger name="org.apache.zookeeper" level="INFO"/>
    <logger name="org.springframework.cloud.client.discovery" level="INFO"/>
    <logger name="com.dangdang.ddframe.job" level="INFO"/>
    <logger name="org.apache.ibatis" level="INFO"/>
    <logger name="org.apache.http" level="INFO"/>
    <logger name="org.apache.shiro" level="INFO"/>
    <logger name="org.mybatis" level="INFO"/>

    o.a.shiro.mg
    <!--druid如果执行sql出错打印日志,这个直接打印到async-default中，这样如果有出错也不会打印到common error中从而污染common error -->
    <logger name="druid.sql.Statement" level="WARN" additivity="false">
        <appender-ref ref="async-default"/>
    </logger>

    <!-- ROOT -->
    <root level="${log.level}">
        <appender-ref ref="console"/>
        <appender-ref ref="async-default"/>
        <appender-ref ref="common-error"/>
    </root>

</configuration>